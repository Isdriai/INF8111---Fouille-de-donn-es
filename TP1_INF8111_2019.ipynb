{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF8111 - Fouille de données\n",
    "\n",
    "\n",
    "## TP1 Automne 2019 - Preprocessing de tweets pour de l'analyse de sentiments\n",
    "\n",
    "##### Membres de l'équipe:\n",
    "\n",
    "    - Rodolphe Picot (1993713)\n",
    "    - Amine Badaoui (1980268)\n",
    "    - Omar Talbi   (1912353)\n",
    "    \n",
    "références:\n",
    "- www.ntlk.org\n",
    "- https://scikit-learn.org/\n",
    "- http://zetcode.com/python/prettytable/\n",
    "- https://matplotlib.org/\n",
    "- https://spacy.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Présentation\n",
    "\n",
    "Twitter est un réseau social permettant aux utilisateurs de publier des informations et communiquer entre eux par le biais de messages, appelés tweets, pouvant contenir jusqu'à 280 caractères. Largement utilisé aujourd'hui, ce réseau peut être un outil pour des entreprises qui souhaitent évaluer l'avis de leurs clients.\n",
    "\n",
    "Dans ce TP, on se met à la place d'une compagnie aérienne, qui souhaiterait détecter les tweets qui la mentionnent et analyser si ce sont des mentions positives ou négatives, en comparant leur résultat avec les autres compagnies.\n",
    "\n",
    "Le *preprocessing* est une tâche cruciale en fouille de données. Elle permet de transformer les données brutes en un format adapté à l'application de méthodes de machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# I/ Analyse de sentiments (13 Pts)\n",
    "\n",
    "Usuellement dans la littérature, la tâche d'extraire le sentiment d'un texte est appelé *sentiment analysis*.\n",
    "Ici pour se faire, nous allons utiliser un modèle *bag-of-words* (BoW).\n",
    "\n",
    "## 1. Installation\n",
    "\n",
    "Pour ce TP, vous aurez besoin des librairies `numpy`, `sklearn` et `scipy` (que vous avez sans doute déjà), ainsi que la librairie `nltk`, qui est une libraire utilisée pour faire du traitement du language (Natural Language Processing, NLP)\n",
    "\n",
    "Installez les libraires en question et exécutez le code ci-dessous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/svetlana/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/svetlana/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/svetlana/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/svetlana/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want, you can use anaconda and install after nltk library\n",
    "#!pip install --user numpy\n",
    "#!pip install --user sklearn\n",
    "#!pip install --user scipy\n",
    "#!pip install --user nltk\n",
    "\n",
    "import sys\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"universal_tagset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Jeu de données\n",
    "\n",
    "On utilise un jeu de donnée provenant de *Crowdflower's Data for Everyone library*: https://www.figure-eight.com/data-for-everyone/\n",
    "\n",
    "Pour citer la source originale de la base :\n",
    "\n",
    "    A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\").\n",
    "\n",
    "Les compagnies incluses dans cette base de données sont Virgin America, United Airline, Southwest Airlines, jetBlue, USAirways, et American Airlines.\n",
    "\n",
    "Dans le fichier zip du TP, vous trouverez le fichier *airline_tweets_database.csv*, qui est la base de données de tweets que nous allons manipuler.\n",
    "\n",
    "Chaque ligne de ce fichier contient un tweet, avec plusieurs informations : l'identifiant du tweet, l'utilisateur, le contenu, le nombre de retweet... Ainsi que le label.\n",
    "\n",
    "3 labels différents sont possibles dans ce dataset : *négatif*, *neutre* et *positif*, que l'on va représenter respectivement par 0, 1 et 2.\n",
    "\n",
    "Pour ce TP, on ne va conserver ici que le texte et le label. On divise ensuite la base de données en 3 ensembles (entrainement/validation/test). Vous utiliserez l'ensemble d'entraînement et de validation pour cette partie, et l'ensemble de test à la partie suivante.\n",
    "\n",
    "Le code ci-dessous permet de charger ces ensembles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training set :  10204\n",
      "Length of validation set :  2240\n",
      "Length of test set :  2196\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_dataset(path):\n",
    "    \n",
    "    texts = []\n",
    "    scores = []\n",
    "\n",
    "    with open(path, 'r', newline='', encoding=\"latin-1\") as csvfile:\n",
    "        \n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        \n",
    "        # Taking the header of the file + the index of useful columns:\n",
    "        header = next(reader)\n",
    "        ind_label = header.index('airline_sentiment')\n",
    "        ind_text = header.index('text')\n",
    "        \n",
    "        for row in reader:\n",
    "            texts.append(row[ind_text])\n",
    "            \n",
    "            label = row[ind_label]\n",
    "            \n",
    "            if label == \"negative\":\n",
    "                scores.append(0)\n",
    "            elif label == \"neutral\":\n",
    "                scores.append(1)\n",
    "            elif label == \"positive\":\n",
    "                scores.append(2)\n",
    "\n",
    "        assert len(texts) == len(scores)\n",
    "\n",
    "        return texts, scores\n",
    "\n",
    "\n",
    "# Path of the dataset\n",
    "path = \"data/airline_tweets_database.csv\"\n",
    "\n",
    "texts_data, scores_data = load_dataset(path)\n",
    "\n",
    "train_valid_X, test_X, train_valid_Y, test_Y = train_test_split(texts_data, scores_data, test_size=0.15, random_state=12)\n",
    "\n",
    "train_X, valid_X, train_Y, valid_Y = train_test_split(train_valid_X, train_valid_Y, test_size=0.18, random_state=12)\n",
    "print(\"Length of training set : \", len(train_X))\n",
    "print(\"Length of validation set : \", len(valid_X))\n",
    "print(\"Length of test set : \", len(test_X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing\n",
    "\n",
    "Nous allons ici implémenter la *tokenization* et le *stemming*, qui sont 2 étapes courantes de preprocessing en NLP. Ensuite, afin d'avoir un modèle qui s'adapte mieux au format de Twitter, nous ajouterons une étape spécifique supplémentaire.\n",
    "\n",
    "### 3.1. Tokenization\n",
    "\n",
    "Cette étape permet de séparer un texte en séquence de *tokens* (= jetons, ici des mots, symboles ou ponctuation).\n",
    "\n",
    "Par exemple la phrase *\"It's the student's notebook.\"* peut être séparé en liste de tokens de cette manière: [\"it\", \" 's\", \"the\", \"student\", \" 's\", \"notebook\", \".\"].\n",
    "\n",
    "**De plus, tous les tokenizers ont également le rôle de mettre le texte en minuscule.**\n",
    "\n",
    "\n",
    "##### Question 1. Implémentez les 2 tokenizers différents suivants: (0.5 Pts)\n",
    "\n",
    "- Le **SpaceTokenizer**, qui est un tokenizer naïf qui sépare simplement en fonction des espaces.\n",
    "- Le **NLTKTokenizer**, qui utilise la méthode du package *nltk* (https://www.nltk.org/api/nltk.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: it's ok \n",
      " no ?\n",
      "Space token: [\"it's\", 'ok', 'no', '?']\n",
      "NLTK token: ['it', \"'s\", 'ok', 'no', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "class SpaceTokenizer(object):\n",
    "    \"\"\"\n",
    "    It tokenizes the tokens that are separated by whitespace (space, tab, newline). \n",
    "    We consider that any tokenization was applied in the text when we use this tokenizer.\n",
    "\n",
    "    For example: \"hello\\tworld of\\nNLP\" is split in ['hello', 'world', 'of', 'NLP']\n",
    "    \"\"\"\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        import re\n",
    "        # on fait attention à ne pas de token vide\n",
    "        return list(filter(lambda token: token != \"\", re.split(' |\\n|\\t', text)))\n",
    "\n",
    "\n",
    "class NLTKTokenizer(object):\n",
    "    \"\"\"\n",
    "    This tokenizer uses the default function of nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
    "    \"\"\"\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        from nltk.tokenize import word_tokenize        \n",
    "        return word_tokenize(text)\n",
    "    \n",
    "text = \"it's ok \\n no ?\"\n",
    "\n",
    "print(\"Text: \" + text)\n",
    "print(\"Space token: \" + str(SpaceTokenizer().tokenize(text)))\n",
    "\n",
    "nltk_tokeniser = NLTKTokenizer()\n",
    "nltk_tokens = nltk_tokeniser.tokenize(text)\n",
    "print(\"NLTK token: \" + str(nltk_tokens))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testez les deux tokenizers. Quelles différences pouvez-vous constater?\n",
    "\n",
    "Les mots ayant comme séparation un autre caractère que espace forment\n",
    "un seul mot avec le mot les précédant pour le tokenizer naïf.\n",
    "\n",
    "Alors qu'avec le NLTK tokenizer, il a une analyse plus fine des séparations entre les mots dans le langage naturel, si on prend l'exemple suivant\n",
    "Hi Mr. Smith today.\n",
    "[\"Mr.\"] sera consédiré comme un token alors today. sera considéré comme [\"today\",\".\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Troncature (ou Stemming)\n",
    "\n",
    "Dans les phrases \"I should have bought a new shoes today\" et \"I spent too much money buying games\", les mots \"buying\" et \"bought\" représentent la même idée. Considérer ces deux mots comme différents ne ferait qu'augmenter pour rien la complexité et la dimension du problème, ce qui peut avoir un impact négatif sur la performance globale. Ainsi, on peut donc plutôt une forme unique (comme la racine du mot) pour représenter ces deux mots de la même manière. Ce procédé de conversion de mots en racines permettant de réduire la dimension est appelé usuellement *stemming*, que l'on peut traduire par troncature.\n",
    "\n",
    "\n",
    "#### Question 2. Récupérez les troncatures des tokens en utilisant l'attribut *stemmer* de la classe *Stemmer* (0.5 Pts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'should', 'have', 'bought', 'a', 'new', 'shoe', 'today', 'i', 'spent', 'too', 'much', 'money', 'buy', 'game']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "class Stemmer(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "    def stem(self, tokens):\n",
    "        \"\"\"\n",
    "        tokens: a list of strings\n",
    "        \"\"\"\n",
    "        return [self.stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "text_stem = \"I should have bought a new shoes today I spent too much money buying games\"\n",
    "\n",
    "tokens = nltk_tokeniser.tokenize(text_stem)\n",
    "\n",
    "print(Stemmer().stem(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Twitter preprocessing\n",
    "\n",
    "Parfois, appliquer uniquement ces deux étapes ne suffit pas, due aux particularités des données que nous manipulons, qui peuvent demander des étapes de preprocessing spécifiques afin d'obtenir un modèle plus adapté.\n",
    "\n",
    "Couramment en NLP, un dictionnaire est utilisé pour stocker un ensemble de mots, et tous les mots n'appartenant pas au dictionnaire sont considérés comme inconnus. Ainsi, avec ce choix d'implémentation, la dimension de l'espace caractéristique du modèle est directement liée au nombre de mots du dictionnaire. Ainsi, pour des raisons de complexité mais aussi car les modèles à trop grande dimension peuvent souffrir du fléau de la dimensionnalité, il est préférable de réduire la taille de notre vocabulaire.\n",
    "\n",
    "#### Question 3. Donnez, en expliquant brièvement, au moins deux exemples d'étapes de préprocessing qui permettent de réduire la taille du dictionnaire ici, puis implémentez-les.  (2.0 points)\n",
    "\n",
    "Ces étapes de préprocessing doivent être en rapport aux charactéristiques spécifiques des données de Twitter. La suppression des mots vides ne compte pas comme une des deux étapes.\n",
    "\n",
    "\n",
    "\n",
    "Réponse: Twitter permet de taguer des personnes ou mettre des hashtags, nous pouvons déjà enlever ou labelliser de manière spécifique des mots commençant par '@' ou '#'. Donc avec la phrase \"@toto tu vas bien ? #FeteDeLHuma\" ressortir \"tu vas bien\" ou \"TwitterAccount tu vas bien ? Festival\"\n",
    "\n",
    "Comme de nombreux réseaux sociaux, on peut ajouter des smileys dans les tweets. On pourrait donc les énumérer et les remplacer par un mot caractérisant l'émotion du smiley. Dans la même idée, on peut ajouter les abréviations, par exemple \"WTF\" marquerait la surprise.\n",
    "\n",
    "On peut aussi remplacer les liens par le mot \"URL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coucou TWITTERACCOUNT tu vas bien  ) viens à la HASHTAG voici le lien pour le ticket URL\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "class TwitterPreprocessing(object):\n",
    "\n",
    "    def preprocess(self, tweet):\n",
    "        \"\"\"\n",
    "        tweet: original tweet\n",
    "        \"\"\"\n",
    "        import re\n",
    "        # on considère que les pseudos twitter n'ont pas de caractères spéciaux\n",
    "        tweet_preproc = re.sub(\"@[a-zA-Z0-9]*\", \"TWITTERACCOUNT\", tweet)\n",
    "        tweet_preproc = re.sub(\"[,.:;!?]\", \"\", tweet_preproc)\n",
    "        \n",
    "        #Pour le moment nous allons juste enlever les hashtags sans les traiter \n",
    "        tweet_preproc = re.sub(\"#[a-zA-Z0-9]*\", \"HASHTAG\", tweet_preproc)\n",
    "                \n",
    "        #Regex naïve pour trouver les URLs, son but est seulement de montrer un exemple\n",
    "        regexURL = \"www.[a-zA-Z0-9]*.[a-zA-Z]*\"\n",
    "        tweet_preproc = re.sub(regexURL, \"URL\", tweet_preproc)\n",
    "              \n",
    "        #De même en ce qui concerne les smileys\n",
    "        smileys = {\":\\)\" : \"HAPPY\",\n",
    "                   \":-\\)\": \"HAPPY\"\n",
    "                  }\n",
    "        \n",
    "        for smiley in smileys:\n",
    "            tweet_preproc = re.sub(smiley, smileys[smiley], tweet_preproc)\n",
    "        \n",
    "        \n",
    "        return tweet_preproc\n",
    "\n",
    "    \n",
    "tweet_test = \"coucou @Toto, tu vas bien ? :) viens à la #FeteDeLHuma, voici le lien pour le ticket www.ticketfetedelhumafake.fr\"\n",
    "twitter_preproc = TwitterPreprocessing()\n",
    "\n",
    "print(twitter_preproc.preprocess(tweet_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.  Pipeline\n",
    "\n",
    "Une pipeline permet d'exécuter séquentiellement toutes les étapes de preprocessing, pour transformer les données brutes en une version utilisable pour notre modèle. La *PreprocessingPipeline* a été implémenter pour appliquer à la suite le tokenizer, les troncatures et le preprocessing spécifique à Twitter. \n",
    "\n",
    "**N'hésitez pas à changer l'ordre des étapes de preprocessing si vous le souhaitez.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingPipeline:\n",
    "\n",
    "    def __init__(self, tokenization, twitterPreprocessing, stemming):\n",
    "        \"\"\"\n",
    "        tokenization: enable or disable tokenization.\n",
    "        twitterPreprocessing: enable or disable twitter preprocessing.\n",
    "        stemming: enable or disable stemming.\n",
    "        \"\"\"\n",
    "\n",
    "        self.tokenizer = NLTKTokenizer() if tokenization else SpaceTokenizer()\n",
    "        self.twitterPreprocesser = TwitterPreprocessing(\n",
    "        ) if twitterPreprocessing else None\n",
    "        self.stemmer = Stemmer() if stemming else None\n",
    "\n",
    "    def preprocess(self, tweet):\n",
    "        \"\"\"\n",
    "        Transform the raw data\n",
    "\n",
    "        tokenization: boolean value.\n",
    "        twitterPreprocessing: boolean value. Apply the\n",
    "        stemming: boolean value.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.twitterPreprocesser:\n",
    "            tweet = self.twitterPreprocesser.preprocess(tweet)\n",
    "\n",
    "        tokens = self.tokenizer.tokenize(tweet)\n",
    "                \n",
    "        if self.stemmer:\n",
    "            tokens = self.stemmer.stem(tokens)\n",
    "            \n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test de la pipeline :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['twitteraccount', 'tell', 'me', 'to', 'talk', 'to', 'twitteraccount', 'about', 'my', 'delay', 'flight', 'aa', 'tell', 'me', 'to', 'talk', 'to', 'us', 'hashtag']]\n"
     ]
    }
   ],
   "source": [
    "pipeline = PreprocessingPipeline(tokenization=True, twitterPreprocessing=True, stemming=True)\n",
    "print(list(map(pipeline.preprocess, train_X[:1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. N-grams\n",
    "\n",
    "Un n-gram est une séquence continue de *n* tokens dans un texte. Par exemple, les séquences *\"nous a\"* et *\"la porte\"* sont deux exemples de 2-grams de la phrase *\"Il nous a dit au revoir en franchissant la porte.\"*. 1-gram, 2-gram et 3-gram sont respectivement appelés unigram, bigram et trigram. \n",
    "\n",
    "Voici la liste de tous les unigrams, bigrams et trigrams possible pour la phrase *\"Il nous a dit au revoir en franchissant la porte.\"* :\n",
    "- Unigram: ['Il', 'nous', 'a', 'dit', 'au', 'revoir', 'en', 'franchissant', 'la', 'porte']\n",
    "- Bigram: ['Il nous', 'nous a', 'a dit', 'dit au', 'au revoir', 'revoir en', 'en franchissant', 'franchissant la', 'la porte']\n",
    "- Trigram: ['Il nous a', 'nous a dit', 'a dit au', 'dit au revoir', 'au revoir en', 'revoir en franchissant', 'en franchissant la', 'franchissant la porte']\n",
    "\n",
    "\n",
    "##### Question 4. Implementez les fonctions `bigram` et `trigram`. (1 Pt)\n",
    "\n",
    "Vous devez résoudre cette question sans utiliser de libraire exterieur comme scikit-learn par exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams(tokens, n):\n",
    "    \"\"\"\n",
    "    tokens: a list of strings\n",
    "    n an integer\n",
    "    \"\"\"\n",
    "    grams = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        grams.append(\" \".join(tokens[i:i + n]))\n",
    "    return grams\n",
    "\n",
    "\n",
    "def bigram(tokens):\n",
    "    \"\"\"\n",
    "    tokens: a list of strings\n",
    "    \"\"\"\n",
    "    return n_grams(tokens, 2)\n",
    "\n",
    "\n",
    "def trigram(tokens):\n",
    "    \"\"\"\n",
    "    tokens: a list of strings\n",
    "    \"\"\"\n",
    "    return n_grams(tokens, 3)\n",
    "\n",
    "tokens = ['Il', 'nous', 'a', 'dit', 'au', 'revoir', 'en', 'franchissant', 'la', 'porte']\n",
    "res_big = ['Il nous', 'nous a', 'a dit', 'dit au', 'au revoir', 'revoir en', 'en franchissant', 'franchissant la', \n",
    "           'la porte']\n",
    "\n",
    "res_tri =  ['Il nous a', 'nous a dit', 'a dit au', 'dit au revoir', 'au revoir en', 'revoir en franchissant',\n",
    "            'en franchissant la', 'franchissant la porte']\n",
    "\n",
    "assert bigram(tokens) == res_big\n",
    "assert trigram(tokens) == res_tri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bag-of-words\n",
    "\n",
    "Régressions logistiques, SVM et d'autres modèles très courants demande des entrées qui soient toutes de la même taille, ce qui n'est forcément le cas pour des types de données comme les textes, qui peuvent avoir un nombre variable de mots.  \n",
    "\n",
    "Par exemple, considérons la phrase 1, ”Board games are much better than video games” et la phrase 2, ”Pandemic is an awesome game!”. La table ci-dessous montre un exemple d'un moyen de représentation de ces deux phrases en utilisant une représentation fixe : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<i></i>     | an | are | ! | pandemic | awesome | better | games | than | video | much | board | is | game |\n",
    "|------------|----|-----|---|----------|---------|--------|-------|------|-------|------|-------|----|------|\n",
    "| Sentence 1 | 0  | 1   | 0 | 0        | 0       | 1      | 2     | 1    | 1     | 1    | 1     | 0  | 0    |\n",
    "| Sentence 2 | 1  | 0   | 0 | 1        | 1       | 0      | 0     | 0    | 0     | 0    | 0     | 1  | 1    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaque colonne représente un mot du vocabulaire (de longueur 13), tandis que chaque ligne contient l'occurence des mots dans une phrase. Ainsi, la valeur 2 à la position (1,7) est due au fait que le mot *\"games\"* apparait deux fois dans la phrase 1. \n",
    "\n",
    "Ainsi, chaque ligne étant de longueur 13, on peut les utiliser comme vecteur pour représenter les phrases 1 et 2. Ainsi, c'est cette méthode que l'on appelle *Bag-of-Words* : c'est une représentation de documents par des vecteurs dont la dimension est égale à la taille du vocabulaire, et qui est construit en comptant le nombre d'occurence de chaque mot. Ainsi, chaque token est ici associé à une dimension.\n",
    "\n",
    "\n",
    "##### Question 5. Implémentez le Bag-of-Words  (2 Pts)\n",
    "\n",
    "Pour cette question, vous ne pouvez pas utiliser de librairie Python externe comme scikit-learn, hormis si vous avez des problèmes de mémoire, vous pouvez utiliser la classe sparse.csr_matrix de scipy (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 0, 'love': 1, 'pizza': 2, 'where': 3, 'is': 4, 'bryan': 5, 'in': 6, 'the': 7, 'kitchen': 8, 'if': 9, 'he': 10, \"'s\": 11, 'not': 12, 'bathroom': 13, 'i love': 14, 'love pizza': 15, 'where is': 16, 'is bryan': 17, 'bryan is': 18, 'is in': 19, 'in the': 20, 'the kitchen': 21, 'kitchen if': 22, 'if he': 23, \"he 's\": 24, \"'s not\": 25, 'not in': 26, 'in kitchen': 27, 'kitchen he': 28, \"'s in\": 29, 'in bathroom': 30, 'i love pizza': 31, 'where is bryan': 32, 'bryan is in': 33, 'is in the': 34, 'in the kitchen': 35, 'the kitchen if': 36, 'kitchen if he': 37, \"if he 's\": 38, \"he 's not\": 39, \"'s not in\": 40, 'not in kitchen': 41, 'in kitchen he': 42, \"kitchen he 's\": 43, \"he 's in\": 44, \"'s in bathroom\": 45}\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix, csc_matrix\n",
    "import numpy as np\n",
    "\n",
    "class CountBoW(object):\n",
    "\n",
    "    def __init__(self, pipeline, bigram=False, trigram=False):\n",
    "        \"\"\"\n",
    "        pipelineObj: instance of PreprocesingPipeline\n",
    "        bigram: enable or disable bigram\n",
    "        trigram: enable or disable trigram\n",
    "        words: list of words in the vocabulary\n",
    "        \"\"\"\n",
    "        self.pipeline = pipeline\n",
    "        self.bigram = bigram\n",
    "        self.trigram = trigram\n",
    "        self.words = None\n",
    "\n",
    "    def add_local_dict(self, tokens, local_dict, len_gramms, index):\n",
    "        \n",
    "        gram = \" \".join(tokens[index:index + len_gramms])\n",
    "        if gram in local_dict:\n",
    "            local_dict[gram] = local_dict[gram] + 1\n",
    "        else:\n",
    "            local_dict[gram] = 1\n",
    "        \n",
    "    def computeBoW(self, tokens_X):\n",
    "        \"\"\"\n",
    "        Calcule du BoW, à partir d'un dictionnaire de mots et d'une liste de tweets.\n",
    "        On suppose que l'on a déjà collecté le dictionnaire sur l'ensemble d'entraînement.\n",
    "\n",
    "        Entrée: tokens, une liste de vecteurs contenant les tweets\n",
    "\n",
    "        Return: une csr_matrix\n",
    "        \"\"\"\n",
    "\n",
    "        if self.words is None:\n",
    "            raise Exception(\n",
    "                \"fit_transform() should be called first (no dictionnary available)\"\n",
    "            )\n",
    "\n",
    "        row = []\n",
    "        col = []\n",
    "        data = []\n",
    "\n",
    "        for index_row, tokens in enumerate(tokens_X, start=0):\n",
    "            dict_count_world_tokens = {}\n",
    "            \n",
    "            length_tokens = len(tokens)\n",
    "            for i in range(length_tokens):\n",
    "                \n",
    "                self.add_local_dict(tokens, dict_count_world_tokens, 1, i)        \n",
    "                    \n",
    "                if self.bigram and i < length_tokens - 1: \n",
    "                    self.add_local_dict(tokens, dict_count_world_tokens, 2, i)\n",
    "                        \n",
    "                if self.trigram and i < length_tokens - 2:\n",
    "                    self.add_local_dict(tokens, dict_count_world_tokens, 3, i)\n",
    "            \n",
    "            try:\n",
    "                for gram, count in dict_count_world_tokens.items():\n",
    "                    col.append(self.words[gram])\n",
    "                    row.append(index_row)\n",
    "                    data.append(dict_count_world_tokens[gram])\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "        return csr_matrix((data, (row, col)), shape=(len(tokens_X), len(self.words)))\n",
    "\n",
    "    def generate_dict(self, tokens_x, len_grams):\n",
    "        \n",
    "        if self.words is None:\n",
    "            self.words = {}\n",
    "        \n",
    "        index = len(self.words)\n",
    "        \n",
    "        for tokens in tokens_x:\n",
    "            grams = n_grams(tokens, len_grams)\n",
    "            for gram in grams:\n",
    "                if gram not in self.words:\n",
    "                    self.words[gram] = index\n",
    "                    index = index + 1\n",
    "    \n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        Cette méthode preprocess les données en utilisant la pipeline, ajoute les bigram et trigram\n",
    "        si besoin, et transforme les textes en vecteurs d'entiers.\n",
    "\n",
    "        Entrée: X, une liste de vecteurs contenant les tweets\n",
    "\n",
    "        Return: une csr_matrix\n",
    "        \"\"\"\n",
    "\n",
    "        tokens_x = list(map(self.pipeline.preprocess, X))\n",
    "        \n",
    "        self.generate_dict(tokens_x, 1)\n",
    "                            \n",
    "        if self.bigram:\n",
    "            self.generate_dict(tokens_x, 2)\n",
    "            \n",
    "        if self.trigram:\n",
    "            self.generate_dict(tokens_x, 3)\n",
    "        \n",
    "        return self.computeBoW(tokens_x)\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Cette méthode preprocess les données en utilisant la pipeline, ajoute les bigram et trigram\n",
    "        si besoin, et transforme les textes en vecteurs d'entiers.\n",
    "        Différence avec fit_transform : on suppose qu'on dispose déjà du dictionnaire ici\n",
    "\n",
    "        Entrée: X, une liste de vecteurs contenant les tweets\n",
    "\n",
    "        Return: une csr_matrix\n",
    "        \"\"\"\n",
    "\n",
    "        if self.words is None:\n",
    "            raise Exception(\n",
    "                \"fit_transform() should be called first (no dictionnary available)\"\n",
    "            )\n",
    "\n",
    "        tokens_x = list(map(self.pipeline.preprocess, X))\n",
    "        \n",
    "        return self.computeBoW(tokens_x)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.words = None\n",
    "\n",
    "\n",
    "c = CountBoW(PreprocessingPipeline(True,True,True), True, True)\n",
    "\n",
    "tweets = [\"I love pizza \", \n",
    "          \"Where is Bryan ?\", \n",
    "          \"Bryan is in the kitchen, if he's not in kitchen, he's in bathroom\"]\n",
    "\n",
    "compute = c.fit_transform(tweets)\n",
    "\n",
    "print(c.words)\n",
    "\n",
    "#print(\"coucou\" + str(compute[0][0]))\n",
    "\n",
    "\n",
    "#print(\"\\n\\n\\n\\n\")\n",
    "\n",
    "#test_bow = CountBoW(PreprocessingPipeline(True,True,True), True, True)\n",
    "\n",
    "#compute2 = test_bow.fit_transform(train_X)\n",
    "#print(len(test_bow.words))\n",
    "\n",
    "#print(compute2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. TF-IDF\n",
    "\n",
    "L'utilisation de la fréquence d'apparition brute des mots, comme c'est le cas avec le bag-of-words, peut être problématique. En effet, peu de tokens auront une fréquence très élevée dans un document, et de ce fait, le poids de ces mots sera beaucoup plus grand que les autres, ce qui aura tendance à biaiser l'ensemble des poids. De plus, les mots qui apparaissent dans la plupart des documents n'aident pas à les discriminer. Par exemple, le mot \"*de*\" apparaît dans beaucoup de tweets de la base de données, et pour autant, avoir ce mot en commun ne permet pas de conclure que des tweets sont similaires. Au contraire, le mot \"*génial*\" est plus rare, mais les documents qui contiennent ce mot sont plus susceptibles d'être positif. TF-IDF est donc une méthode qui permet de pallier à ce problème.\n",
    "\n",
    "TF-IDF pondère le vecteur en utilisant une fréquence de document inverse (IDF) et une fréquence de termes (TF).\n",
    "\n",
    "TF est l'information locale sur l'importance qu'a un mot dans un document donné, tandis que IDF mesure la capacité de discrimination des mots dans un jeu de données. \n",
    "\n",
    "L'IDF d'un mot se calcule de la façon suivante:\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\text{idf}_i = \\log\\left( \\frac{N}{\\text{df}_i} \\right),\n",
    "\\end{equation}\n",
    "\n",
    "avec $N$ le nombre de documents dans la base de donnée, et $\\text{df}_i$ le nombre de documents qui contiennent le mot $i$.\n",
    "\n",
    "Le nouveau poids $w_{ij}$ d'un mot $i$ dans un document $j$ peut ensuite être calculé de la façon suivante:\n",
    "\n",
    "\\begin{equation}\n",
    "\tw_{ij} = \\text{tf}_{ij} \\times \\text{idf}_i,\n",
    "\\end{equation}\n",
    "\n",
    "avec $\\text{tf}_{ij}$ la fréquence du mot $i$ dans le document $j$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Question 6. Implémentez le bag-of-words avec la pondération de TF-IDF (3 Pts)\n",
    "\n",
    "Pour cette question, vous ne pouvez pas utiliser de librairie Python externe comme scikit-learn, hormis si vous avez des problèmes de mémoire, vous pouvez utiliser la classe sparse.csr_matrix de scipy (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 0, 'love': 1, 'pizza': 2, 'where': 3, 'is': 4, 'bryan': 5, 'in': 6, 'the': 7, 'kitchen': 8, 'if': 9, 'he': 10, \"'s\": 11, 'not': 12, 'bathroom': 13, 'i love': 14, 'love pizza': 15, 'where is': 16, 'is bryan': 17, 'bryan is': 18, 'is in': 19, 'in the': 20, 'the kitchen': 21, 'kitchen if': 22, 'if he': 23, \"he 's\": 24, \"'s not\": 25, 'not in': 26, 'in kitchen': 27, 'kitchen he': 28, \"'s in\": 29, 'in bathroom': 30, 'i love pizza': 31, 'where is bryan': 32, 'bryan is in': 33, 'is in the': 34, 'in the kitchen': 35, 'the kitchen if': 36, 'kitchen if he': 37, \"if he 's\": 38, \"he 's not\": 39, \"'s not in\": 40, 'not in kitchen': 41, 'in kitchen he': 42, \"kitchen he 's\": 43, \"he 's in\": 44, \"'s in bathroom\": 45}\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "\n",
    "\n",
    "class TFIDFBoW(object):\n",
    "\n",
    "    def __init__(self, pipeline, bigram=False, trigram=False):\n",
    "        \"\"\"\n",
    "        pipelineObj: instance of PreprocesingPipeline\n",
    "        bigram: enable or disable bigram\n",
    "        trigram: enable or disable trigram\n",
    "        words: list of words in the vocabulary\n",
    "        idf: list of idfs for each document\n",
    "        \"\"\"\n",
    "        self.pipeline = pipeline\n",
    "        self.bigram = bigram\n",
    "        self.trigram = trigram\n",
    "        self.words = None\n",
    "        self.idf = None\n",
    "        \n",
    "    def add_local_dict_TFIDF(self, tokens, local_dict, len_gramms, index, local_TFIDF):\n",
    "        \n",
    "        gram = \" \".join(tokens[index:index + len_gramms])\n",
    "        \n",
    "        if gram not in local_TFIDF:\n",
    "            local_TFIDF.append(gram)\n",
    "        if gram in local_dict:\n",
    "            local_dict[gram] = local_dict[gram] + 1\n",
    "        else:\n",
    "            local_dict[gram] = 1\n",
    "        \n",
    "    def computeTFIDF(self, X):\n",
    "        \"\"\"\n",
    "        Calcule du TF-IDF, à partir d'un dictionnaire de mots et d'une \n",
    "        liste de tweets.\n",
    "        On suppose que l'on a déjà collecté le dictionnaire ainsi que \n",
    "        calculé le vecteur contenant l'idf pour chaque document.\n",
    "        \n",
    "        Entrée : X, une liste de vecteurs contenant les tweets\n",
    "        \n",
    "        Return: une csr_matrix\n",
    "        \"\"\"\n",
    "       \n",
    "        row = []\n",
    "        col = []\n",
    "        data = []\n",
    "\n",
    "        self.idf = {}\n",
    "        for index_row, tokens in enumerate(X, start=0):\n",
    "            dict_count_world_tokens = {}\n",
    "            TFIDF_local = []\n",
    "            length_tokens = len(tokens)\n",
    "            for i in range(length_tokens):\n",
    "                \n",
    "                self.add_local_dict_TFIDF(tokens, dict_count_world_tokens, 1, i, TFIDF_local)        \n",
    "                 \n",
    "                if self.bigram and i < length_tokens - 1:  \n",
    "                    self.add_local_dict_TFIDF(tokens, dict_count_world_tokens, 2, i, TFIDF_local)\n",
    "                        \n",
    "                if self.trigram and i < length_tokens - 2:\n",
    "                    self.add_local_dict_TFIDF(tokens, dict_count_world_tokens, 3, i, TFIDF_local)\n",
    "                \n",
    "            try:\n",
    "                for gram, count in dict_count_world_tokens.items():\n",
    "                    \n",
    "                    col.append(self.words[gram])\n",
    "                    row.append(index_row)\n",
    "                    data.append(float(dict_count_world_tokens[gram]))\n",
    "            except KeyError:\n",
    "                continue\n",
    "                \n",
    "            for gram in TFIDF_local:\n",
    "                if gram in self.idf:\n",
    "                    self.idf[gram].append(index_row)\n",
    "                else:\n",
    "                    self.idf[gram] = [index_row]\n",
    "                \n",
    "        matrice = csr_matrix((data, (row, col)), shape=(len(X), len(self.words)))\n",
    "        try:\n",
    "            for gram, col_index in self.words.items():\n",
    "                list_tweet_row = self.idf[gram]\n",
    "                for tweet_row in list_tweet_row:\n",
    "                    matrice[tweet_row, col_index] *= math.log(len(X)/len(list_tweet_row))\n",
    "        except KeyError:\n",
    "            pass\n",
    "        return matrice\n",
    "    \n",
    "    def generate_dict(self, tokens_x, len_grams):\n",
    "        \n",
    "        if self.words is None:\n",
    "            self.words = {}\n",
    "        \n",
    "        index = len(self.words)\n",
    "        \n",
    "        for tokens in tokens_x:\n",
    "            grams = n_grams(tokens, len_grams)\n",
    "            for gram in grams:\n",
    "                if gram not in self.words:\n",
    "                    self.words[gram] = index\n",
    "                    index = index + 1\n",
    "        \n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        Cette méthode preprocess les données en utilisant la pipeline, ajoute les bigram et trigram \n",
    "        si besoin, et transforme les textes en vecteurs de flottants avec la pondération TF-IDF.\n",
    "        \n",
    "        Entrée : X, une liste de vecteurs contenant les tweets\n",
    "        \n",
    "        Return: une csr_matrix\n",
    "        \"\"\"\n",
    "        tokens_x = list(map(self.pipeline.preprocess, X))\n",
    "        \n",
    "        self.generate_dict(tokens_x, 1)\n",
    "                            \n",
    "        if self.bigram:\n",
    "            self.generate_dict(tokens_x, 2)\n",
    "            \n",
    "        if self.trigram:\n",
    "            self.generate_dict(tokens_x, 3)\n",
    "        \n",
    "        return self.computeTFIDF(tokens_x)\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Cette méthode preprocess les données en utilisant la pipeline, ajoute les bigram et trigram \n",
    "        si besoin, et transforme les textes en vecteurs de flottants avec la pondération TF-IDF.\n",
    "        Différence avec fit_transform : on suppose qu'on dispose déjà du dictionnaire et du calcul des idf ici.\n",
    "            \n",
    "        Entrée : X, une liste de vecteurs contenant les tweets\n",
    "        \n",
    "        Return: une csr_matrix\n",
    "        \"\"\"\n",
    "\n",
    "        if self.words is None:\n",
    "            raise Exception(\n",
    "                \"fit_transform() should be called first (no dictionnary available)\"\n",
    "            )\n",
    "\n",
    "        tokens_x = list(map(self.pipeline.preprocess, X))\n",
    "        \n",
    "        return self.computeTFIDF(tokens_x)\n",
    "    \n",
    "    \n",
    "c2 = TFIDFBoW(PreprocessingPipeline(True,True,True), True, True)\n",
    "\n",
    "tweets = [\"I love pizza \"]\n",
    "\n",
    "compute = c2.fit_transform(tweets)\n",
    "\n",
    "print(c.words)\n",
    "\n",
    "#print(compute)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Classification utilisant BoW\n",
    "\n",
    "Pour la classification, nous allons effectuer une régression logisitique (vu en cours ou que vous allez voir bientôt). \n",
    "Pour en savoir plus : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "La méthode `train_evaluate` entraîne et évalue le modèle de régression logistique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def train_evaluate(training_X, training_Y, validation_X, validation_Y, bowObj):\n",
    "    \"\"\"\n",
    "    training_X: tweets from the training dataset\n",
    "    training_Y: tweet labels from the training dataset\n",
    "    validation_X: tweets from the validation dataset\n",
    "    validation_Y: tweet labels from the validation dataset\n",
    "    bowObj: Bag-of-word object\n",
    "    \n",
    "    :return: the classifier and its accuracy in the training and validation dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    classifier = LogisticRegression(n_jobs=-1)\n",
    "\n",
    "    training_rep = bowObj.fit_transform(training_X)\n",
    "   \n",
    "    classifier.fit(training_rep, training_Y)\n",
    "\n",
    "    trainAcc = accuracy_score(training_Y, classifier.predict(training_rep))\n",
    "    validationAcc = accuracy_score(\n",
    "        validation_Y, classifier.predict(bowObj.transform(validation_X)))\n",
    "\n",
    "    return classifier, trainAcc, validationAcc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Question 7. Entraînez et calculez la précision de la régression logistique sur les ensembles d'entraînement et de validation. (4 points)\n",
    "\n",
    "Essayez les configurations suivantes :\n",
    "\n",
    "    1. CountBoW + SpaceTokenizer(without tokenizer) + unigram \n",
    "    2. CountBoW + NLTKTokenizer + unigram\n",
    "    3. TFIDFBoW + NLTKTokenizer + Stemming + unigram\n",
    "    4. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram\n",
    "    5. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram + bigram\n",
    "    6. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram + bigram + trigram\n",
    "\n",
    "Outre la précision, reportez la taille du dictionnaire pour chacune des configurations. Enfin, décrivez vos résultats obtenus et répondez aux questions suivantes:\n",
    "- Quelles étapes de preprocessing ont effectivement aidé le modèle ? Pourquoi ?\n",
    "- La pondération avec TF-IDF a-t-elle aidé à obtenir une meilleure performance que le simple BoW ?\n",
    "- Les bigrams et trigrams ont-ils amélioré la performance ? Expliquez pourquoi.\n",
    "\n",
    "Les étapes de prepocessin qui ont effectivement aidé le modèle sont: \n",
    "        1- Le stemming car il a reduit le nombre de mot possible\n",
    "        2- La pondération avec TF-IDF a aidé à obtenir une meilleure performance que le simple BoW\n",
    "        3- les bigrames ont amélioré la performance car ils ont ajouté un sorte de signification sur les noms\n",
    "        composés, ce qui existe dans la vie réelle ex: new york, thank you contrairement aux trigram qui contrairement aux bigrams, sont beaucoup moins présents dans une langue\n",
    "Indiquez quelle est la configuration que vous choisissez.\n",
    "La configuration choisie\n",
    "TFIDFBoW(PreprocessingPipeline(True,False,True), True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:1544: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComputeBow + Spacetokenizer : (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=-1, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), 0.9613876911015288, 0.9103182256509161)\n",
      "\n",
      "ComputeBow + NLTKTokenizer + unigram : (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=-1, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), 0.9428655429243434, 0.9006750241080038)\n",
      "\n",
      "TFIDFBoW + NLTKTokenizer + Stemming + unigram : (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=-1, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), 0.9884359074872598, 0.929122468659595)\n",
      "\n",
      "TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming + unigram : (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=-1, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), 0.9843198745589965, 0.9260687881710061)\n",
      "\n",
      "TFIDFBoW + NLTKTokenizer + Stemming + unigram + bigram : (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=-1, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), 0.9979419835358683, 0.9074252651880425)\n",
      "\n",
      "TFIDFBoW + NLTKTokenizer + Stemming + unigram + bigram + trigram : (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=-1, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), 0.9970599764798118, 0.8845226615236258)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "b1 = CountBoW(PreprocessingPipeline(False,False,False), False, False)\n",
    "print(\"ComputeBow + Spacetokenizer : \"+str(train_evaluate(train_X, train_Y, train_valid_X, train_valid_Y , b1))+\"\\n\")\n",
    "\n",
    "b2 = CountBoW(PreprocessingPipeline(True,False,False), False, False)\n",
    "print(\"ComputeBow + NLTKTokenizer + unigram : \"+str(train_evaluate(train_X, train_Y, train_valid_X, train_valid_Y , b2))+\"\\n\")\n",
    "\n",
    "b3 = TFIDFBoW(PreprocessingPipeline(True,False,True), False, False)\n",
    "print(\"TFIDFBoW + NLTKTokenizer + Stemming + unigram : \"+str(train_evaluate(train_X, train_Y, train_valid_X, train_valid_Y , b3))+\"\\n\")\n",
    "\n",
    "b4 = TFIDFBoW(PreprocessingPipeline(True,True,True), False, False)\n",
    "print(\"TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming + unigram : \"+str(train_evaluate(train_X, train_Y, train_valid_X, train_valid_Y , b4))+\"\\n\")\n",
    "\n",
    "b5 = TFIDFBoW(PreprocessingPipeline(True,False,True), True, False)\n",
    "print(\"TFIDFBoW + NLTKTokenizer + Stemming + unigram + bigram : \"+str(train_evaluate(train_X, train_Y, train_valid_X, train_valid_Y , b5))+\"\\n\")\n",
    "\n",
    "b6 = TFIDFBoW(PreprocessingPipeline(True,True,True), True, True)\n",
    "print(\"TFIDFBoW + NLTKTokenizer + Stemming + unigram + bigram + trigram : \"+str(train_evaluate(train_X, train_Y, train_valid_X, train_valid_Y , b6))+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II/ Prototype (7 points)\n",
    "\n",
    "Maintenant que nous avons un modèle de classification entraîné pour l'analyse de sentiments, nous pouvons l'appliquer à notre ensemble de tests et analyser le résultat.\n",
    "\n",
    "## 1. Analyse de Sentiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 9. Implémentez la fonction `detect_airline` qui détecte la compagnie aérienne d'un tweet. (1,5 points)\n",
    "\n",
    "Expliquez votre approche, et les inconvénients possibles.\n",
    "\n",
    "**Attention :** `detect_airline` doit être en mesure de gérer le cas où aucune compagnie n'est mentionnée (auquel cas `None` est retounée), mais aussi le cas où plusieurs compagnies sont mentionnées dans un tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['usairways', 'americanair', 'southwestair', 'jetblue', 'united', 'virginamerica', 'imaginedragons', 'delta']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" L'inconvénient de cette approche est que \\nl'on peut identifier un compte comme une compagnie aérienne alors que ce n'est pas le cas\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def detect_airline(tweets):\n",
    "    \n",
    "    \"\"\"\n",
    "    Detect and return the airline companies mentioned in the tweet\n",
    "    \n",
    "    tweet: represents the tweet message. You should define the data type\n",
    "    \n",
    "    Return: list of detected airline companies\n",
    "    \"\"\"\n",
    "    \"\"\"\" créer un dictionnaire de touts les mots qui commencent par @ et calculer leurs frequences d'appartition \n",
    "    sur l'ensemble des tweets en utilise quelque chose qui ressemble à computbow avec un ensemble d'entrainement.\n",
    "    puis on calcule la moyenne d'apprition des mots qui commencent avec @, et en dernier on vérifier si la fréquence \n",
    "    d'un mot est supérieur à la moyenne on l'ajoute. cette méthode donne de bons résultats mais déscrimine les \n",
    "    comapgnie qui apparaissent peu.\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"Nous avons un peu modifié la signature de detect_airline pour pouvoir faire une analyse plus générale\"\"\"\n",
    "\n",
    "list_companie={}\n",
    "def analyze_account(tweets):\n",
    "    \n",
    "    import re\n",
    "    list_account_occ={}\n",
    "    results=[]\n",
    "     \n",
    "    for tweet in tweets:\n",
    "\n",
    "        accounts= re.findall(r'@[a-zA-Z0-9]+',tweet)\n",
    "        \n",
    "        for account in accounts:\n",
    "            account= re.sub('@', \"\", account)\n",
    "            account=account.lower()\n",
    "            # Nous comptons le nombre d'occurence d'un compte dans le total des tweets (1)\n",
    "            if account in list_account_occ:\n",
    "                list_account_occ[account]=list_account_occ[account]+1\n",
    "            else:\n",
    "                list_account_occ[account]=1\n",
    "    \n",
    "    #print(list_accounts)\n",
    "    #print(list_account_occ)\n",
    "    somme=0\n",
    "    l=len(list_account_occ)\n",
    "    # Nous comptons la moyenne d'occurence d'un compte (2)\n",
    "    for _, occ in list_account_occ.items():\n",
    "        somme=occ+somme\n",
    "    moyenne=somme/l\n",
    "    #print(moyenne)\n",
    "    \n",
    "    # Et considérons que ce compte est une compagnie si il dépasse la moyenne d'occurence des comptes (3)\n",
    "    for account, occ in list_account_occ.items():    \n",
    "        if occ >= moyenne:\n",
    "            results.append(account)\n",
    "    \n",
    "    return results\n",
    "    \n",
    "\n",
    "a= [\"@virgin I hate you\",\"@aircanada offers tickets for @ladygaga concerts\",\n",
    "    \"@virgin is cooperating with @airamerica on an incident on the boeing 747\"]\n",
    "print(analyze_account(train_X))\n",
    "\n",
    "\"\"\" L'inconvénient de cette approche est que \n",
    "l'on peut identifier un compte comme une compagnie aérienne alors que ce n'est pas le cas\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Question 10. Implémentez la fonction `extract_sentiment` qui, à partir de tweets et d'un classificateur, extrait leurs sentiments. (0.5 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def extract_sentiment():\n",
    "    \"\"\"\n",
    "    Extract the tweet sentiment\n",
    "    \n",
    "    classifier: classifier object\n",
    "    tweet: represents the tweet message. You should define the data type\n",
    "    \n",
    "    Return: list of detected airline companies\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #print(test_X[:1])\n",
    "    c3 = TFIDFBoW(PreprocessingPipeline(True,False,True), True, False)\n",
    "    \n",
    "    classifier = LogisticRegression() # sinon classifier\n",
    "\n",
    "    training_rep = c3.fit_transform(train_X)\n",
    "   \n",
    "    classifier.fit(training_rep, train_Y) # entrainement\n",
    "  \n",
    "    \n",
    "    #print(\"fin\")\n",
    "    return classifier.predict(c3.transform(test_X))\n",
    "    # TODO\n",
    "\n",
    "extract_sentiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 11. En utilisant `extract_tweet_content`, `detect_airline` et `extract_sentiment`, générez un diagramme en bar contenant le nombre de tweets positives, négatifs et neutres pour chacune des compagnies. (2 points)\n",
    "\n",
    "Décrivez brièvement le diagramme et analysez les résultats (par exemple, quelle est la compagnie avec le plus de tweets négatifs?). Expliquez comment un tel diagramme peut aider des compagnies aériennes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\\nLe diagramme représente le nombre de tweets négatifs, neutres et positifs pour chaque compagnie identifiée, on remarque \\nque la le sentiment neutre a une distribution plutot importantes pour la plupart des compagnies selon notre extraction. \\nsuivi du sentiment négatif qui lui aussi apparait assez souvent sur les tweets car les personnes ont assez la tendance \\nà exprimer leurs quotidiens de mauvaise expérience vis à vis des entreprise via twitter. On remarque que usaairways\\na eu beaucoup de retour négatif selon nos données. ce diagrame peut aider les compagnies à cibler les améliorations \\nde leurs services on identifiant l'information récupérer sur les tweets et on faisant la comparaisons avec leurs \\ninformations pour réduire les situations de déagremment futur, aussi ce diagrame peut etre un utilisé comme état initial \\npour connaitre l'amélioration\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(len(analyze_account(train_X)))\n",
    "#https://chrisalbon.com/python/data_visualization/matplotlib_grouped_bar_plot/\n",
    "companies = analyze_account(train_X) # C'est notre detect_airline\n",
    "l=len(companies)\n",
    "Negatif=[0]*l\n",
    "Neutre=[0]*l\n",
    "Positif=[0]*l\n",
    "\n",
    "result_test=extract_sentiment()\n",
    "\n",
    "#print(result_test)\n",
    "i=0\n",
    "for compagnie in companies:\n",
    "    j=0\n",
    "    for tweet in test_X: \n",
    "        #print(tweet)\n",
    "        if compagnie in tweet.lower():\n",
    "            if(result_test[j])==0:\n",
    "                Negatif[i]=Negatif[i]+1\n",
    "            if(result_test[j])==1:\n",
    "                Neutre[i]=Neutre[i]+1\n",
    "            if(result_test[j])==2:\n",
    "                Positif[i]=Positif[i]+1\n",
    "        j=j+1\n",
    "    i=i+1\n",
    "#print(Negatif)\n",
    "#print(Neutre)\n",
    "#print(Positif)\n",
    "                \n",
    "#!pip install --user matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "barWidth=0.2\n",
    "\n",
    "r1=np.arange(len(Negatif))\n",
    "r2=[x+barWidth for x in r1]\n",
    "r3=[x+barWidth for x in r2]\n",
    "\n",
    "plt.bar(r1, Negatif, color='#EE3224', width=barWidth, edgecolor='white', label='Negatif')\n",
    "plt.bar(r2, Neutre, color='#F78F1E', width=barWidth, edgecolor='white', label='Neutre')\n",
    "plt.bar(r3, Positif, color='#32CD32', width=barWidth, edgecolor='white', label='Positif')\n",
    "\n",
    "plt.xlabel('Compagnies', fontweight='bold')\n",
    "plt.xticks([r + barWidth for r in range(len(Negatif))], companies)\n",
    "plt.rcParams[\"figure.figsize\"]=[30,15]\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "Le diagramme représente le nombre de tweets négatifs, neutres et positifs pour chaque compagnie identifiée, on remarque \n",
    "que la le sentiment neutre a une distribution plutot importantes pour la plupart des compagnies selon notre extraction. \n",
    "suivi du sentiment négatif qui lui aussi apparait assez souvent sur les tweets car les personnes ont assez la tendance \n",
    "à exprimer leurs quotidiens de mauvaise expérience vis à vis des entreprise via twitter. On remarque que usaairways\n",
    "a eu beaucoup de retour négatif selon nos données. ce diagrame peut aider les compagnies à cibler les améliorations \n",
    "de leurs services on identifiant l'information récupérer sur les tweets et on faisant la comparaisons avec leurs \n",
    "informations pour réduire les situations de déagremment futur, aussi ce diagrame peut etre un utilisé comme état initial \n",
    "pour connaitre l'amélioration\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyse de termes\n",
    "\n",
    "Le POS-tagging (pour *part-of-speech tagging*, en français étiquetage grammatical) consiste à l'extraction de l'information grammaticale d'un token dans une phrase. Par exemple, la table ci-dessous donne un exemple du *POS-tagging* de la phrase *\"The cat is white!\"*\n",
    "\n",
    "\n",
    "|   The   | cat  |  is  | white     |    !       |\n",
    "|---------|------|------|-----------|------------|\n",
    "| article | noun | verb | adjective | punctation |\n",
    "\n",
    "\n",
    "Pour autant, le *POS-tagging* peut être plus complexe que les règles simples apprises à l'école. Il faut souvent des informations plus détaillées sur le rôle d'un terme dans une phrase. Pour notre problème, nous n'avons pas besoin d'utiliser un modèle linguistique plus complexe, nous allons utiliser ce qu'on appelle des *POS-tags* universelles.\n",
    "\n",
    "En *POS-tagging*, chaque token est représenté par un tag. La liste des POS-tags utilisés sont disponibles ici :\n",
    "https://universaldependencies.org/u/pos/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The cat', 'NOUN'), ('cat is', 'VERB'), ('is white', 'ADJ'), ('white !', 'NOUN')]\n",
      "The cat\n",
      "cat is\n",
      "is white\n",
      "white !\n"
     ]
    }
   ],
   "source": [
    "# NLTK POS-tagger\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "#before using pos_tag function, you have to tokenize the sentence.\n",
    "s = ['The', 'cat', 'is',  'white', '!']\n",
    "s=bigram(s)\n",
    "\n",
    "r=nltk.pos_tag(s,tagset='universal')\n",
    "print(r)\n",
    "for s in r:\n",
    "    print(s[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 12. Implémentez un code qui collecte les 10 termes les plus fréquents pour chaque compagnie aérienne. (2 Pts)\n",
    "\n",
    "Ici, vous n'allez considérer que les termes apparaissant dans les tweets positifs et négatifs. \n",
    "\n",
    "De plus, nous allons utiliser la définition suivante de \"terme\":\n",
    "\n",
    "1. Un mot qui est soit un adjectif, soit un nom\n",
    "2. Un N-gram composé d'adjectifs suivit par un nom (par exemple, \"nice place\"), ou un nom suivi par un autre nom (par exemple, \"sports club\").\n",
    "\n",
    "Ensuite, **générez une table** contenant les 10 termes les plus fréquents, avec leurs fréquences (en pourcentage) pour chaque compagnie.\n",
    "\n",
    "*N'oubliez pas de supprimer le nom de la compagnie parmi les termes !*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+---------+----------+-----+-------+---------+-------+-----+-----------+------+\n",
      "|  Compagnie  | time | service | customer | due | phone | weather | great | one | thank you | http |\n",
      "+-------------+------+---------+----------+-----+-------+---------+-------+-----+-----------+------+\n",
      "| americanair |  14  |    13   |    13    |  12 |   10  |    10   |   8   |  8  |     8     |  8   |\n",
      "+-------------+------+---------+----------+-----+-------+---------+-------+-----+-----------+------+\n",
      "+--------------+------+--------+---------+-------+----------+------------------+------+--------+----------+------------+\n",
      "|  Compagnie   | hold | flight | service | hours | customer | customer service | http | http : | flight . | Thanks for |\n",
      "+--------------+------+--------+---------+-------+----------+------------------+------+--------+----------+------------+\n",
      "| southwestair |  14  |   11   |    11   |   11  |    9     |        8         |  7   |   7    |    7     |     6      |\n",
      "+--------------+------+--------+---------+-------+----------+------------------+------+--------+----------+------------+\n",
      "+-----------+-------+------+------+------+--------+------------------+---------+------+---------+--------+\n",
      "| Compagnie | plane | time | seat | http | http : | customer service | flights | hold | airline | people |\n",
      "+-----------+-------+------+------+------+--------+------------------+---------+------+---------+--------+\n",
      "|   united  |   12  |  11  |  10  |  10  |   10   |        9         |    9    |  9   |    8    |   8    |\n",
      "+-----------+-------+------+------+------+--------+------------------+---------+------+---------+--------+\n",
      "+-----------+---------+--------+-----------+------+--------+-----------+----------+------------------+-------+-------+\n",
      "| Compagnie | service | flight | thank you | http | http : | Thank you | customer | customer service | plane | fleet |\n",
      "+-----------+---------+--------+-----------+------+--------+-----------+----------+------------------+-------+-------+\n",
      "|  jetblue  |    9    |   8    |     7     |  7   |   7    |     6     |    6     |        5         |   4   |   4   |\n",
      "+-----------+---------+--------+-----------+------+--------+-----------+----------+------------------+-------+-------+\n",
      "+-----------+------+---------+-----+------------------+------+------+---------+------+----------+-------+\n",
      "| Compagnie | hold | service | amp | customer service | time | gate | airline | hour | hold for | phone |\n",
      "+-----------+------+---------+-----+------------------+------+------+---------+------+----------+-------+\n",
      "| usairways |  24  |    17   |  17 |        16        |  15  |  14  |    13   |  13  |    13    |   12  |\n",
      "+-----------+------+---------+-----+------------------+------+------+---------+------+----------+-------+\n",
      "+---------------+--------+------+--------+------+------+----------+-----+----+------+----------------+\n",
      "|   Compagnie   | flight | http | http : | site | book | goodness | nwk | gt | boom | thank goodness |\n",
      "+---------------+--------+------+--------+------+------+----------+-----+----+------+----------------+\n",
      "| virginamerica |   3    |  3   |   3    |  2   |  2   |    1     |  1  | 1  |  1   |       1        |\n",
      "+---------------+--------+------+--------+------+------+----------+-----+----+------+----------------+\n"
     ]
    }
   ],
   "source": [
    "def term(tokens):\n",
    "    r=nltk.pos_tag(tokens,tagset='universal')\n",
    "    \n",
    "from prettytable import PrettyTable\n",
    "table= PrettyTable()\n",
    "\n",
    "import re\n",
    "def ten_terms_company(tweets,results,compagnies):\n",
    "    lists_dict_companies={}\n",
    "    words={}\n",
    "    for compagnie in compagnies:# rechercher les tweet positif ou negatif dans lesqules les compagnies apparaissent\n",
    "        i=0                      # ex; {americanair:[5,7}] => americanair apparait sur le tweet 5 et 7.\n",
    "        local_compagnie=[]\n",
    "        \n",
    "        for tweet in tweets:\n",
    "            if(results[i]==2) or (results[i]==0): # tweet positive ou négatif\n",
    "                if compagnie in tweet.lower():\n",
    "                    if compagnie in lists_dict_companies:\n",
    "                        lists_dict_companies[compagnie].append(i)\n",
    "                    \n",
    "                    if compagnie not in local_compagnie:\n",
    "                        local_compagnie.append(compagnie)\n",
    "                        lists_dict_companies[compagnie]=[i]\n",
    "\n",
    "            i=i+1\n",
    "    #print(lists_dict_companies)\n",
    "    l=1\n",
    "    for compagnie in lists_dict_companies:\n",
    "        list_local_word_by_company={}\n",
    "        for row in lists_dict_companies[compagnie]:\n",
    "            tweet= re.sub(compagnie, \"\", tweets[row])\n",
    "            tweet= re.sub('@', \"\", tweets[row])\n",
    "            tokens = nltk_tokeniser.tokenize(tweet)\n",
    "            # add also the bigram\n",
    "            b=bigram(tokens)\n",
    "            r=nltk.pos_tag(tokens,tagset='universal')\n",
    "            \n",
    "            \n",
    "            q=0\n",
    "            while q<len(r):\n",
    "                if((r[q][0]).lower()not in lists_dict_companies):\n",
    "                    if((r[q][1])==\"NOUN\" or (r[q][1])==\"ADJ\" and (r[q][0]).lower()!=compagnie):\n",
    "\n",
    "                        if(r[q][0] not in list_local_word_by_company ):\n",
    "                            list_local_word_by_company[r[q][0].lower()]=1\n",
    "                        else:\n",
    "                            list_local_word_by_company[r[q][0].lower()]=list_local_word_by_company[r[q][0]]+1\n",
    "\n",
    "                q=q+1\n",
    "\n",
    "            \n",
    "            \n",
    "            for s in b:\n",
    "                tok=nltk_tokeniser.tokenize(s)\n",
    "                r_gram=nltk.pos_tag(tok,tagset='universal')\n",
    "                if((r_gram[0][0]).lower()not in lists_dict_companies and (r_gram[1][0]).lower()not in lists_dict_companies):\n",
    "                    if((r_gram[0][1])==\"NOUN\" or (r_gram[0][1])==\"ADJ\" and ((r_gram[1][1])==\"NOUN\")):\n",
    "                        if(s not in list_local_word_by_company ):\n",
    "                            list_local_word_by_company[s]=1\n",
    "                        else:\n",
    "                            list_local_word_by_company[s]=list_local_word_by_company[s]+1\n",
    "        #print(list_local_word_by_company)\n",
    "         \n",
    "        import operator\n",
    "        sorted_dictionary = sorted(list_local_word_by_company.items(), key=operator.itemgetter(1),reverse=True)\n",
    "        col=[\"Compagnie\"]\n",
    "        col1=[compagnie]\n",
    "        for al in sorted_dictionary[:10]:\n",
    "            col.append(al[0])\n",
    "            col1.append(al[1])\n",
    "        #print(col1)\n",
    "        table.field_names=col\n",
    "        table.add_row(col1)\n",
    "        print(table)\n",
    "        table.clear_rows()\n",
    "      \n",
    "    \n",
    "ten_terms_company(test_X,extract_sentiment(),analyze_account(test_X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 13. Que conclure de la table généré à la question 12 pour les compagnies ? (1 Pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Les termes les plus influancant sur le sentiment du tweet pour chaque compagnie, par exemple thank you, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III/ Bonus (2 points)\n",
    "\n",
    "Les noms de personnes, les noms de sociétés et les emplacements sont appelés \"entités nommées\". La reconnaissance d'entité nommée (NER, pour *Named-entity recognition*) consiste à extraire les entités nommées en les classant à l'aide de catégories prédéfinies. Dans cette section bonus, vous utiliserez un outil de NER pour extraire automatiquement des entités nommées des tweets. Cette approche est suffisamment générique pour récupérer des informations sur d’autres sociétés ou même des noms de produits et de personnes.\n",
    "\n",
    "\n",
    "**Pour le bonus, vous êtes libres d'utiliser n'importe quel NER implémenté en Python.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question Bonus 1.  Implémentez un code qui génère une table contenant le top 10 des NER de la base de données. (1 point)\n",
    "\n",
    "Cette table doit contenir les fréquences des entités nommées. Ensuite, générez un diagramme en bar qui montre le nombre de tweets positifs, négatifs ou neutres pour chacunes des 10 NER. Décrivez le résultat obtenu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('@JetBlue', 81), ('Cancelled Flightled', 71), ('today', 63), ('2', 52), ('tomorrow', 29), ('Late Flight', 29), ('first', 28), ('United', 26), ('4', 25), ('Cancelled Flighted', 24)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "a= [\"@virgin I hate you Apple\",\"@aircanada offers tickets U.K. for @ladygaga concerts\",\n",
    "    \"@virgin is cooperating with @airamerica on an incident on the boeing 747\",\n",
    "    \"Apple is looking at buying U.K. startup for $1 billion\"]\n",
    "\n",
    "list_NER10={}\n",
    "def analyze_NER(a):\n",
    "    \n",
    "  \n",
    "    \n",
    "    list_NER={}\n",
    "    list_NER_occ={}\n",
    "    results=[]\n",
    "    i=0\n",
    "\n",
    "     \n",
    "    for tweet in a:\n",
    "\n",
    "        tokens = nlp(tweet)\n",
    "        NER=tokens.ents\n",
    "        #print(NER)\n",
    "        for ner in NER:\n",
    "            ner=str(ner)\n",
    "            if(ner!=\"#\"):\n",
    "                if ner in list_NER_occ:\n",
    "                    list_NER_occ[ner]=list_NER_occ[ner]+1\n",
    "                else:\n",
    "                    list_NER_occ[ner]=1\n",
    "    #print(list_NER_occ)\n",
    "    import operator\n",
    "    sorted_dictionary = sorted(list_NER_occ.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    #print(sorted_dictionary[1][0])\n",
    "    return(sorted_dictionary[:10])\n",
    "\n",
    "\n",
    "\n",
    "#analyze_NER(train_X)\n",
    "\n",
    "#print(len(analyze_account(train_X)))\n",
    "#https://chrisalbon.com/python/data_visualization/matplotlib_grouped_bar_plot/\n",
    "res=analyze_NER(test_X)\n",
    "print(res)\n",
    "\n",
    "result_test=extract_sentiment()\n",
    "\n",
    "#print(result_test)\n",
    "Negatif=[0]*10\n",
    "Neutre=[0]*10\n",
    "Positif=[0]*10\n",
    "\n",
    "#result_test=extract_sentiment()\n",
    "\n",
    "print(result_test)\n",
    "i=0\n",
    "for ner,b in res:\n",
    "    j=0\n",
    "    for tweet in test_X:\n",
    "        #print(tweet)\n",
    "        if ner in tweet:\n",
    "            if(result_test[j])==0:\n",
    "                Negatif[i]=Negatif[i]+1\n",
    "            if(result_test[j])==1:\n",
    "                Neutre[i]=Neutre[i]+1\n",
    "            if(result_test[j])==2:\n",
    "                Positif[i]=Positif[i]+1\n",
    "        j=j+1\n",
    "    i=i+1\n",
    "    \n",
    "#print(Negatif)\n",
    "#print(Neutre)\n",
    "#print(Positif)\n",
    "                \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "barWidth=0.2\n",
    "\n",
    "r1=np.arange(len(Negatif))\n",
    "r2=[x+barWidth for x in r1]\n",
    "r3=[x+barWidth for x in r2]\n",
    "\n",
    "plt.bar(r1, Negatif, color='#EE3224', width=barWidth, edgecolor='white', label='Negatif')\n",
    "plt.bar(r2, Neutre, color='#F78F1E', width=barWidth, edgecolor='white', label='Neutre')\n",
    "plt.bar(r3, Positif, color='#32CD32', width=barWidth, edgecolor='white', label='Positif')\n",
    "\n",
    "plt.xlabel('NER', fontweight='bold')\n",
    "plt.xticks([r + barWidth for r in range(len(Negatif))], analyze_NER(test_X))\n",
    "plt.rcParams[\"figure.figsize\"]=[30,15]\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question Bonus 2. Générez une table similaire à la question 12 pour le top 10 des NER pour chaque compagnie. (1 point)\n",
    "\n",
    "Que peut-on conclure de ces résultats?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------+-------+----------+---+--------+----+-------------+-----+--------------+---------+\n",
      "|  Compagnie  | cancelled flightled | today | tomorrow | 2 | dallas | aa | late flight | mia | over an hour | tonight |\n",
      "+-------------+---------------------+-------+----------+---+--------+----+-------------+-----+--------------+---------+\n",
      "| americanair |          17         |   8   |    8     | 6 |   4    | 4  |      4      |  3  |      3       |    3    |\n",
      "+-------------+---------------------+-------+----------+---+--------+----+-------------+-----+--------------+---------+\n",
      "+--------------+---------------------+------------------+---------+-----+-----+-------+--------+-----------+--------------+-----+\n",
      "|  Compagnie   | cancelled flightled | cancelled flight | 2 hours | atl | bna | today | 2 days | southwest | this morning | mdw |\n",
      "+--------------+---------------------+------------------+---------+-----+-----+-------+--------+-----------+--------------+-----+\n",
      "| southwestair |          8          |        5         |    3    |  3  |  3  |   3   |   2    |     2     |      2       |  2  |\n",
      "+--------------+---------------------+------------------+---------+-----+-----+-------+--------+-----------+--------------+-----+\n",
      "+-----------+-----+---+-------+-----+-------------+-------+---------+----+-----+---+\n",
      "| Compagnie | sfo | 2 | first | iah | late flight | worst | houston | ua | two | 1 |\n",
      "+-----------+-----+---+-------+-----+-------------+-------+---------+----+-----+---+\n",
      "|   united  |  8  | 7 |   6   |  4  |      4      |   3   |    3    | 3  |  3  | 3 |\n",
      "+-----------+-----+---+-------+-----+-------------+-------+---------+----+-----+---+\n",
      "+-----------+----------+-------+-----+--------------+---------------------+-------------+-----------------------+---------+------------------+-----+\n",
      "| Compagnie | @jetblue | today | jfk | the next day | cancelled flightled | late flight | which day of the week | pandora | cancelled flight | bos |\n",
      "+-----------+----------+-------+-----+--------------+---------------------+-------------+-----------------------+---------+------------------+-----+\n",
      "|  jetblue  |    14    |   2   |  2  |      2       |          2          |      2      |           1           |    1    |        1         |  1  |\n",
      "+-----------+----------+-------+-----+--------------+---------------------+-------------+-----------------------+---------+------------------+-----+\n",
      "+-----------+---------------------+-------+---+----+-------------+---------+-------+--------------------+--------------+------------+\n",
      "| Compagnie | cancelled flightled | today | 2 | us | late flight | 4 hours | first | cancelled flighted | late flightr | last night |\n",
      "+-----------+---------------------+-------+---+----+-------------+---------+-------+--------------------+--------------+------------+\n",
      "| usairways |          14         |   10  | 8 | 7  |      7      |    4    |   4   |         4          |      3       |     3      |\n",
      "+-----------+---------------------+-------+---+----+-------------+---------+-------+--------------------+--------------+------------+\n",
      "+---------------+--------+------+---------+---------------+---------+---+-----+-----+-----+----------------+\n",
      "|   Compagnie   | friday | boom | nyc-jfk | san francisco | tonight | 3 | 883 | 413 | jfk | this afternoon |\n",
      "+---------------+--------+------+---------+---------------+---------+---+-----+-----+-----+----------------+\n",
      "| virginamerica |   1    |  1   |    1    |       1       |    1    | 1 |  1  |  1  |  1  |       1        |\n",
      "+---------------+--------+------+---------+---------------+---------+---+-----+-----+-----+----------------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#!pip install spacy\n",
    "#!pip install collections\n",
    "\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from prettytable import PrettyTable\n",
    "table1= PrettyTable()\n",
    "\n",
    "import re\n",
    "def ten_ner_company(tweets,results,compagnies):\n",
    "    lists_dict_companies={}\n",
    "    words={}\n",
    "    for compagnie in compagnies:\n",
    "        i=0\n",
    "        local_compagnie=[]\n",
    "        for tweet in tweets:\n",
    "            if(results[i]==2) or (results[i]==0): # tweet positive\n",
    "                if compagnie in tweet.lower():\n",
    "                    if compagnie in lists_dict_companies:\n",
    "                        lists_dict_companies[compagnie].append(i)\n",
    "                    \n",
    "                    if compagnie not in local_compagnie:\n",
    "                        local_compagnie.append(compagnie)\n",
    "                        lists_dict_companies[compagnie]=[i]\n",
    "\n",
    "            i=i+1\n",
    "    #print(lists_dict_companies)   \n",
    "    for compagnie in lists_dict_companies:\n",
    "        list_local_word_by_company={}\n",
    "        for row in lists_dict_companies[compagnie]:\n",
    "            tweet= re.sub(compagnie, \"\", tweets[row])\n",
    "            tweet= re.sub('@', \"\", tweets[row])\n",
    "            tweet= re.sub('#', \"\", tweets[row])\n",
    "            tokens = nlp(tweet)\n",
    "            tokens_ent=tokens.ents\n",
    "            # add also the bigram\n",
    "            #r=nltk.pos_tag(tokens,tagset='universal')\n",
    "            \n",
    "            #print(r)\n",
    "            q=0\n",
    "            while q<len(tokens_ent):\n",
    "                if(str((tokens_ent[q])).lower() not in lists_dict_companies):\n",
    "                    if(str((tokens_ent[q])).lower()!=compagnie):\n",
    "                        if(str(tokens_ent[q]).lower() in list_local_word_by_company ):\n",
    "                            list_local_word_by_company[str(tokens_ent[q]).lower()]=list_local_word_by_company[str(tokens_ent[q]).lower()]+1\n",
    "                        else:\n",
    "                            list_local_word_by_company[str(tokens_ent[q]).lower()]=1\n",
    "                    \n",
    "                q=q+1\n",
    "        #print(compagnie+\"=================\")\n",
    "        import operator\n",
    "        sorted_dictionary = sorted(list_local_word_by_company.items(), key=operator.itemgetter(1),reverse=True)\n",
    "        col=[\"Compagnie\"]\n",
    "        col1=[compagnie]\n",
    "        for al in sorted_dictionary[:10]:\n",
    "            \n",
    "            col.append(al[0])\n",
    "            col1.append(al[1])\n",
    "        #print(col1)\n",
    "        table1.field_names=col\n",
    "        table1.add_row(col1)\n",
    "        print(table1)\n",
    "        table1.clear_rows()\n",
    "\n",
    "\n",
    "ten_ner_company(test_X,extract_sentiment(),analyze_account(test_X)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On conclue que la table générée avec spacy (NER) permet de montrer la relation entre un mot et sa participation dans le \\nsentiment dans les tweet, sur la question de réprsentation des compagnies en bar avec les tweets positifs, négatifs \\net neutres nous avons aussi observé que americanair avait beaucoup de tweets négatifs qui est bien repris dans la table\\npour cette comapgnie qui montre une fréquence élevée de cancelled flighted.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''On conclue que la table générée avec spacy (NER) permet de montrer la relation entre un mot et sa participation dans le \n",
    "sentiment dans les tweet, sur la question de réprsentation des compagnies en bar avec les tweets positifs, négatifs \n",
    "et neutres nous avons aussi observé que americanair avait beaucoup de tweets négatifs qui est bien repris dans la table\n",
    "pour cette comapgnie qui montre une fréquence élevée de cancelled flighted.'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
