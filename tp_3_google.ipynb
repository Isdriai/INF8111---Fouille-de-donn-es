{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from itertools import combinations\n",
    "import time as t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_product(row):\n",
    "    \"\"\"\n",
    "    Map each transaction into a set of KEY-VALUE elements.\n",
    "    The KEY is the word (product) itself and the VALUE is its number of apparitions.\n",
    "    \"\"\"\n",
    "    products = row.transaction.split(';') # split products from the column transaction\n",
    "    for p in products:\n",
    "        yield (p, 1)\n",
    "\n",
    "def reduce_product_by_key(value1, value2):\n",
    "    \"Reduce the mapped objects to unique words by merging (summing ) their values\"\n",
    "    return value1+value2\n",
    "\n",
    "\n",
    "def format_tuples(pattern):\n",
    "    \"\"\"\n",
    "    Used for visualizition.\n",
    "    Transforms tuples to a string since Dataframe does not support column of tuples with different sizes\n",
    "    (a,b,c) -> '(a,b,c)'\n",
    "    \"\"\"\n",
    "   \n",
    "    return (str(tuple(pattern[0])), str(pattern[1]))\n",
    "\n",
    "# split_function permet d'avoir plus de flexibilité au niveau des données d'entrée\n",
    "def map_to_patterns(products, split_function = lambda x: x, max_products_by_pattern = 3):\n",
    "    for i in range(1, max_products_by_pattern+1): # [1;4[\n",
    "        for c in combinations(split_function(products), i):\n",
    "            yield (c, 1) \n",
    "\n",
    "def reduce_product_by_key(value1, value2):\n",
    "   \n",
    "    return value1+value2\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "def map_to_subpatterns(pattern):\n",
    "    key = list(pattern[0])\n",
    "    value = pattern[1]\n",
    "    n = len(key)\n",
    "    yield (tuple(key), (None, value))\n",
    "\n",
    "    if n > 1:\n",
    "        for i in range(n):\n",
    "            new_key = deepcopy(key)\n",
    "            remove = new_key.pop(i)\n",
    "            yield (tuple(new_key), (remove,value))\n",
    "\n",
    "            \n",
    "def map_to_assoc_rules(rule):\n",
    "    for prod, value in rule[1]:\n",
    "        if prod == None:\n",
    "            tot = float(value)\n",
    "            break\n",
    "        \n",
    "    confidence = (rule[0], [(prod, value/tot) for prod, value in rule[1] if prod != None ])\n",
    "    yield confidence\n",
    "\n",
    "    \n",
    "def MBA(rdd):\n",
    "    return rdd.flatMap(lambda row: map_to_patterns(row.Transaction))\\\n",
    "                    .reduceByKey(reduce_product_by_key) \\\n",
    "                    .flatMap(map_to_subpatterns) \\\n",
    "                    .groupByKey().mapValues(list) \\\n",
    "                    .flatMap(map_to_assoc_rules)\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "\n",
    "df_order_prior = spark.read.csv('gs://bucket_tp_3_picot/order_products__prior.csv', header=True, sep=',', inferSchema=True)\n",
    "df_order_prior.createOrReplaceTempView(\"order_prod_p\") # creates table 'order_prod'\n",
    "\n",
    "start_p = int(round(t.time() * 1000))\n",
    "results = spark.sql('SELECT COLLECT_LIST(opp.product_id) AS Transaction' \n",
    "               ' FROM order_prod_p opp GROUP BY order_id'\n",
    "          )\n",
    "end_p = int(round(t.time() * 1000))\n",
    "print((end_p - start_p) / 1000 )\n",
    "results.show(5, truncate=80)\n",
    "\n",
    "start = int(round(t.time() * 1000))\n",
    "assoc = MBA(results.rdd)\n",
    "assoc.map(format_tuples).toDF(['patterns', 'association_rules']).show(5)\n",
    "end = int(round(t.time() * 1000))\n",
    "\n",
    "time = (end - start) / 1000 \n",
    "\"\"\"Time is in seconds\"\"\"\n",
    "print(time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
